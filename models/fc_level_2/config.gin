import code.residual_ppo

# Parameters for pi/Adam:
# ==============================================================================
pi/Adam.amsgrad = False
pi/Adam.betas = (0.9, 0.999)
pi/Adam.eps = 1e-05
pi/Adam.lr = 5e-05
pi/Adam.weight_decay = 0

# Parameters for vf/Adam:
# ==============================================================================
vf/Adam.amsgrad = False
vf/Adam.betas = (0.9, 0.999)
vf/Adam.eps = 1e-05
vf/Adam.lr = 0.0001
vf/Adam.weight_decay = 0

# Parameters for Checkpointer:
# ==============================================================================
Checkpointer.ckpt_period = 100000
Checkpointer.format = '{:09d}'

# Parameters for DiagGaussian:
# ==============================================================================
DiagGaussian.log_std_max = 2
DiagGaussian.log_std_min = -20

# Parameters for make_pybullet_env:
# ==============================================================================
make_pybullet_env.action_space = 'torque'
make_pybullet_env.frameskip = 3
make_pybullet_env.initializer = 'task2_init'
make_pybullet_env.monitor = False
make_pybullet_env.norm_observations = True
make_pybullet_env.residual = True
make_pybullet_env.reward_fn = 'task2_reward_reg_slippage'
make_pybullet_env.seed = 0
make_pybullet_env.termination_fn = 'stay_close_to_goal'
make_pybullet_env.visualization = False

# Parameters for Policy:
# ==============================================================================
# None.

# Parameters for policy_fn:
# ==============================================================================
policy_fn.torque_std = 0.01

# Parameters for ResidualPPO2:
# ==============================================================================
ResidualPPO2.alpha = 1.5
ResidualPPO2.batch_size = 512
ResidualPPO2.ent_coef = 0.0
ResidualPPO2.env_fn = @make_pybullet_env
ResidualPPO2.epochs_pi = 10
ResidualPPO2.epochs_vf = 10
ResidualPPO2.eval_num_episodes = 100
ResidualPPO2.gamma = 0.99
ResidualPPO2.gpu = True
ResidualPPO2.kl_target = 0.01
ResidualPPO2.lambda_ = 0.95
ResidualPPO2.max_grad_norm = 5.0
ResidualPPO2.nenv = 32
ResidualPPO2.norm_advantages = True
ResidualPPO2.opt_pi = @pi/optim.Adam
ResidualPPO2.opt_vf = @vf/optim.Adam
ResidualPPO2.policy_fn = @policy_fn
ResidualPPO2.policy_training_start = 1000000
ResidualPPO2.record_num_episodes = 0
ResidualPPO2.rollout_length = 256
ResidualPPO2.value_fn = @value_fn

# Parameters for train:
# ==============================================================================
train.algorithm = @ResidualPPO2
train.eval = True
train.eval_period = 10000000
train.maxseconds = None
train.maxt = 100000000
train.save_period = 1000000
train.seed = 0

# Parameters for value_fn:
# ==============================================================================
# None.

# Parameters for ValueFunction:
# ==============================================================================
# None.

# Parameters for VecObsNormWrapper:
# ==============================================================================
VecObsNormWrapper.eps = 0.01
VecObsNormWrapper.log = True
VecObsNormWrapper.log_prob = 0.01
VecObsNormWrapper.mean = None
VecObsNormWrapper.std = None
VecObsNormWrapper.steps = 10000
